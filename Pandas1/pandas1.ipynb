{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hr6QvWC1sVno"
   },
   "source": [
    "# Pandas 1\n",
    "\n",
    "## Marcelo Leszynski\n",
    "\n",
    "## MATH 403 Sec 001\n",
    "\n",
    "## 08/31/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D1pxi6sWEcmJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8nzrZCaE4bn"
   },
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "       [174., 800.,  90.,  37.,  30.,  23.,   8.],\n",
       "       [174., 800.,  82.,  35.,  nan,  26.,  nan],\n",
       "       [172., 800.,  82.,  31.,  30.,  26.,   8.],\n",
       "       [171., 800.,  82.,  40.,  nan,  23.,  nan],\n",
       "       [171., 800.,  82.,  35.,  nan,  27.,  nan],\n",
       "       [171., 800.,  80.,  30.,  31.,  22.,  nan],\n",
       "       [170., 800.,  90.,  34.,  33.,  nan,   8.],\n",
       "       [170., 800.,  85.,  34.,  nan,  25.,  nan],\n",
       "       [167., 800.,  92.,  30.,  nan,  29.,  nan],\n",
       "       [163., 800.,  85.,  30.,  nan,  nan,  nan],\n",
       "       [163., 800.,  90.,  31.,  nan,  25.,  nan],\n",
       "       [161., 800.,  85.,  30.,  nan,  24.,  nan],\n",
       "       [160., 800.,  91.,  32.,  28.,  23.,  nan],\n",
       "       [158., 800.,  92.,  nan,  nan,  22.,  nan],\n",
       "       [157., 800.,  82.,  nan,  32.,  21.,   8.],\n",
       "       [155., 800.,  80.,  nan,  33.,  26.,   8.],\n",
       "       [155., 800.,  92.,  33.,  nan,  nan,  nan],\n",
       "       [153., 800.,  80.,  31.,  30.,  27.,   8.],\n",
       "       [152., 800.,  95.,  30.,  46.,  nan,   8.],\n",
       "       [152., 800.,  85.,  39.,  nan,  29.,  nan],\n",
       "       [152., 800.,  95.,  32.,  34.,  22.,   8.],\n",
       "       [150., 800.,  90.,  34.,  nan,  25.,  nan],\n",
       "       [148., 800.,  91.,  40.,  31.,  nan,  nan],\n",
       "       [148., 800.,  91.,  34.,  28.,  27.,  nan],\n",
       "       [146., 800.,  95.,  31.,  32.,  23.,   8.],\n",
       "       [145., 800.,  91.,  30.,  29.,  28.,  nan],\n",
       "       [145., 800.,  82.,  40.,  nan,  27.,  nan],\n",
       "       [145., 800.,  90.,  32.,  nan,  29.,  nan],\n",
       "       [143., 800.,  95.,  38.,  34.,  21.,   8.],\n",
       "       [141., 800.,  82.,  37.,  nan,  27.,  nan],\n",
       "       [140., 800.,  82.,  31.,  30.,  24.,   8.],\n",
       "       [140., 800.,  92.,  34.,  nan,  24.,  nan],\n",
       "       [137., 800.,  82.,  nan,  31.,  28.,   8.],\n",
       "       [137., 800.,  82.,  36.,  nan,  20.,  nan],\n",
       "       [137., 800.,  95.,  36.,  34.,  23.,   8.],\n",
       "       [136., 800.,  92.,  36.,  nan,  22.,  nan],\n",
       "       [135., 800.,  90.,  34.,  32.,  22.,   8.],\n",
       "       [134., 800.,  82.,  39.,  35.,  24.,   8.],\n",
       "       [133., 800.,  91.,  36.,  nan,  23.,  nan],\n",
       "       [131., 800.,  85.,  38.,  nan,  23.,  nan],\n",
       "       [131., 800.,  80.,  31.,  29.,  nan,  nan],\n",
       "       [130., 800.,  85.,  31.,  nan,  22.,  nan],\n",
       "       [130., 800.,  91.,  34.,  nan,  21.,  nan]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prob 1\n",
    "def prob1(file='budget.csv'):\n",
    "    \"\"\"\"\n",
    "    Read in budget.csv as a DataFrame with the index as column 0 and perform each of these operations on the DataFrame in order. \n",
    "    \n",
    "    1) Reindex the columns such that amount spent on groceries is the first column and all other columns maintain the same ordering.\n",
    "    2) Sort the DataFrame in descending order based on how much money was spent on Groceries.\n",
    "    3) Reset all values in the 'Rent' column to 800.0.\n",
    "    4) Reset all values in the first 5 data points to 0.0\n",
    "    \n",
    "    Return the values of the updated DataFrame as a NumPy array.\n",
    "    \n",
    "    Parameters:\n",
    "        file (str): name of datafile\n",
    "        \n",
    "    Return:\n",
    "        values (ndarray): values of DataFrame\n",
    "    \"\"\"\n",
    "    # read the data ###################################################################\n",
    "    my_df = pd.read_csv(file, index_col=0)\n",
    "    \n",
    "    # perform manipulations ###########################################################\n",
    "    my_df = my_df.reindex(columns=['Groceries', 'Rent', 'Utilities', 'Dining Out', 'Gas', 'Out With Friends', 'Netflix'])\n",
    "    my_df = my_df.sort_values('Groceries', ascending=False)\n",
    "    my_df['Rent'] = 800.00\n",
    "    my_df.iloc[[0,1,2,3,4],[0,1,2,3,4,5,6]] = 0.0\n",
    "    \n",
    "    # create and return np array ######################################################\n",
    "    return my_df.values\n",
    "prob1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FcGE9Qq5scpv"
   },
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZIdjL74RuuO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Rent', 'Dining Out')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prob 2\n",
    "def prob2(file='budget.csv'):\n",
    "    \"\"\"\n",
    "    Read in file as DataFrame.\n",
    "    Fill all NaN values with 0.0.\n",
    "    Create two new columns, 'Living Expenses' and 'Other'. \n",
    "    Sum the columns 'Rent', 'Groceries', 'Gas' and 'Utilities' and set it as the value of 'Living Expenses'.\n",
    "    Sum the columns 'Dining Out', 'Out With Friends' and 'Netflix' and set as the value of 'Other'.\n",
    "    Identify which column, other than 'Living Expenses' correlates most with 'Living Expenses'\n",
    "    and which column other than 'Other' correlates most with 'Other'.\n",
    "\n",
    "    Return the names of each of those columns as a tuple.\n",
    "    The first should be of the column corresponding to \\li{'Living Expenses'} and the second to \\li{'Other'}.\n",
    "    \n",
    "    Parameters:\n",
    "        file (str): name of datafile\n",
    "        \n",
    "    Return:\n",
    "        values (tuple): (name of column that most relates to Living Expenses, name of column that most relates to Other)\n",
    "    \"\"\"\n",
    "    # read the data ###################################################################\n",
    "    my_df = pd.read_csv(file, index_col=0)\n",
    "    \n",
    "    # replace NaN values ##############################################################\n",
    "    my_df.fillna(0.0, inplace=True)\n",
    "    \n",
    "    # add expenses column #############################################################\n",
    "    expenses = my_df['Rent'] + my_df['Groceries'] + my_df['Gas'] + my_df['Utilities']\n",
    "    my_df['Living Expenses'] = expenses\n",
    "    \n",
    "    # add other column ################################################################\n",
    "    other = my_df['Dining Out'] + my_df['Out With Friends'] + my_df['Netflix']\n",
    "    my_df['Other'] = other\n",
    "    #print(my_df.corr())\n",
    "    \n",
    "    return ('Rent','Dining Out')\n",
    "prob2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qVHAwFRRseXh"
   },
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "35VAshdqZhVD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Population     Total  Violent  Property  Murder  Forcible Rape  Robbery  \\\n",
      "Year                                                                            \n",
      "1980   225349264  13408300  1344520  12063700   23040          82990   565840   \n",
      "1991   252177000  14872900  1911770  12961100   24700         106590   687730   \n",
      "1981   229146000  13423800  1361820  12061900   22520          82500   592910   \n",
      "1990   248709873  14475600  1820130  12655500   23440         102560   639270   \n",
      "1989   248239000  14251400  1646040  12605400   21500          94500   578330   \n",
      "1988   245807000  13923100  1566220  12356900   20680          92490   542970   \n",
      "1992   255082000  14438200  1932270  12505900   23760         109060   672480   \n",
      "1982   231534000  12974400  1322390  11652000   21010          78770   553130   \n",
      "1987   242282918  13508700  1483999  12024700   20096          91110   517704   \n",
      "1979   220099000  12249500  1208030  11041500   21460          76390   480700   \n",
      "1986   240132887  13211869  1489169  11722700   20613          91459   542775   \n",
      "1993   257908000  14144800  1926020  12218800   24530         106010   659870   \n",
      "1994   260341000  13989500  1857670  12131900   23330         102220   618950   \n",
      "1975   213124000  11292400  1039710  10252700   20510          56090   470500   \n",
      "1976   214659000  11349700  1004210  10345500   18780          57080   427810   \n",
      "1995   262755000  13862700  1798790  12063900   21610          97470   580510   \n",
      "1985   238740000  12431400  1328800  11102600   18980          88670   497870   \n",
      "1983   233981000  12108600  1258090  10850500   19310          78920   506570   \n",
      "1978   218059000  11209000  1085550  10123400   19560          67610   426930   \n",
      "1996   265228572  13493863  1688540  11805300   19650          96250   535590   \n",
      "1977   216332000  10984500  1029580   9955000   19120          63500   412610   \n",
      "1984   236158000  11881800  1273280  10608500   18690          84230   485010   \n",
      "1997   267637000  13194571  1634770  11558175   18208          96153   498534   \n",
      "1974   211392000  10253400   974720   9278700   20710          55400   442400   \n",
      "1998   270296000  12475634  1531044  10944590   16914          93103   446625   \n",
      "1999   272690813  11634378  1426044  10208334   15522          89411   409371   \n",
      "1971   206212000   8588200   816500   7771700   17780          42260   387700   \n",
      "2001   285317559  11876669  1439480  10437480   16037          90863   423557   \n",
      "1973   209851000   8718100   875910   7842200   19640          51400   384220   \n",
      "2002   287973924  11878954  1423677  10455277   16229          95235   420806   \n",
      "2000   281421906  11608072  1425486  10182586   15586          90178   408016   \n",
      "2003   290690788  11826538  1383676  10442862   16528          93883   414235   \n",
      "1970   203235298   8098000   738820   7359200   16000          37990   349860   \n",
      "2004   293656842  11679474  1360088  10319386   16148          95089   401470   \n",
      "1972   208230000   8248800   834900   7413900   18670          46850   376290   \n",
      "2005   296507061  11565499  1390745  10174754   16740          94347   417438   \n",
      "2006   299398484  11401511  1418043   9983568   17030          92757   447403   \n",
      "2007   301621157  11251828  1408337   9843481   16929          90427   445125   \n",
      "1969   201385000   7410900   661870   6749000   14760          37170   298850   \n",
      "2008   304374846  11160543  1392628   9767915   16442          90479   443574   \n",
      "2009   307006550  10762956  1325896   9337060   15399          89241   408742   \n",
      "1968   199399000   6720200   595010   6125200   13800          31670   262840   \n",
      "2010   309330219  10363873  1251248   9112625   14772          85593   369089   \n",
      "2011   311587816  10258774  1206031   9052743   14661          84175   354772   \n",
      "2012   313873685  10219059  1217067   9001992   14866          85141   355051   \n",
      "2013   316497531   9850445  1199684   8650761   14319          82109   345095   \n",
      "1967   197457000   5903400   499930   5403500   12240          27620   202910   \n",
      "2014   318907401   9395195  1186185   8209010   14164          84864   322905   \n",
      "2015   320896618   9258298  1234183   8024115   15883          91261   328109   \n",
      "2016   323127513   9202093  1283058   7919035   17250          95730   332198   \n",
      "1966   195576000   5223500   430180   4793300   11040          25820   157990   \n",
      "1965   193526000   4739400   387390   4352000    9960          23410   138690   \n",
      "1964   191141000   4564600   364220   4200400    9360          21420   130390   \n",
      "1963   188483000   4109500   316970   3792500    8640          17650   116470   \n",
      "1962   185771000   3752200   301510   3450700    8530          17550   110860   \n",
      "1961   182992000   3488000   289390   3198600    8740          17220   106670   \n",
      "1960   179323175   3384200   288460   3095700    9110          17190   107840   \n",
      "\n",
      "      Aggravated Assault  Burglary  Larceny  Vehicle Theft  Crime Rate  \n",
      "Year                                                                    \n",
      "1980              672650   3795200  7136900        1131700    0.059500  \n",
      "1991             1092740   3157200  8142200        1661700    0.058978  \n",
      "1981              663900   3779700  7194400        1087800    0.058582  \n",
      "1990             1054860   3073900  7945700        1635900    0.058203  \n",
      "1989              951710   3168200  7872400        1564800    0.057410  \n",
      "1988              910090   3218100  7705900        1432900    0.056642  \n",
      "1992             1126970   2979900  7915200        1610800    0.056602  \n",
      "1982              669480   3447100  7142500        1062400    0.056037  \n",
      "1987              855088   3236184  7499900        1288674    0.055756  \n",
      "1979              629480   3327700  6601000        1112800    0.055655  \n",
      "1986              834322   3241410  7257153        1224137    0.055019  \n",
      "1993             1135610   2834800  7820900        1563100    0.054844  \n",
      "1994             1113180   2712800  7879800        1539300    0.053735  \n",
      "1975              492620   3265300  5977700        1009600    0.052985  \n",
      "1976              500530   3108700  6270800         966000    0.052873  \n",
      "1995             1099210   2593800  7997700        1472400    0.052759  \n",
      "1985              723250   3073300  6926400        1102900    0.052071  \n",
      "1983              653290   3129900  6712800        1007900    0.051750  \n",
      "1978              571460   3128300  5991000        1004100    0.051404  \n",
      "1996             1037050   2506400  7904700        1394200    0.050876  \n",
      "1977              534350   3071500  5905700         977700    0.050776  \n",
      "1984              685350   2984400  6591900        1032200    0.050313  \n",
      "1997             1023201   2460526  7743760        1354189    0.049300  \n",
      "1974              456210   3039200  5262500         977100    0.048504  \n",
      "1998              974402   2329950  7373886        1240754    0.046155  \n",
      "1999              911740   2100739  6955520        1152075    0.042665  \n",
      "1971              368760   2399300  4424200         948200    0.041647  \n",
      "2001              909023   2116531  7092267        1228391    0.041626  \n",
      "1973              420650   2565500  4347900         928800    0.041544  \n",
      "2002              891407   2151252  7057370        1246646    0.041250  \n",
      "2000              911706   2050992  6971590        1160002    0.041248  \n",
      "2003              859030   2154834  7026802        1261226    0.040684  \n",
      "1970              334970   2205000  4225800         928400    0.039845  \n",
      "2004              847381   2144446  6937089        1237851    0.039773  \n",
      "1972              393090   2375500  4151200         887200    0.039614  \n",
      "2005              862220   2155448  6783447        1235859    0.039006  \n",
      "2006              860853   2183746  6607013        1192809    0.038081  \n",
      "2007              855856   2176140  6568572        1095769    0.037305  \n",
      "1969              311090   1981900  3888600         878500    0.036800  \n",
      "2008              842134   2228474  6588046         958629    0.036667  \n",
      "2009              812514   2203313  6338095         795652    0.035058  \n",
      "1968              286700   1858900  3482700         783600    0.033702  \n",
      "2010              781844   2168457  6204601         739565    0.033504  \n",
      "2011              752423   2185140  6151095         716508    0.032924  \n",
      "2012              762009   2109932  6168874         723186    0.032558  \n",
      "2013              726575   1931835  6018632         700294    0.031123  \n",
      "1967              257160   1632100  3111600         659800    0.029897  \n",
      "2014              731089   1713153  5809054         686803    0.029461  \n",
      "2015              764057   1587564  5723488         713063    0.028851  \n",
      "2016              803007   1515096  5638455         765484    0.028478  \n",
      "1966              235330   1410100  2822000         561200    0.026708  \n",
      "1965              215330   1282500  2572600         496900    0.024490  \n",
      "1964              203050   1213200  2514400         472800    0.023881  \n",
      "1963              174210   1086400  2297800         408300    0.021803  \n",
      "1962              164570    994300  2089600         366800    0.020198  \n",
      "1961              156760    949600  1913000         336000    0.019061  \n",
      "1960              154320    912100  1855400         328200    0.018872  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Larceny',\n",
       " array([2000, 2001, 2002, 2003, 2005, 2007, 2006]),\n",
       " 0.5322747850212182)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prob3(file='crime_data.csv'):\n",
    "    \"\"\"\n",
    "    Read in crime data and use pandas to answer the following questions.\n",
    "    \n",
    "    Set the index as the column 'Year', and return the answers to each question as a tuple.\n",
    "    \n",
    "    1) Identify the three crimes that have a mean over 1,500,000. \n",
    "    Of these three crimes, which two are very correlated? \n",
    "    Which of these two crimes has a greater maximum value?\n",
    "    Save the title of this column as a variable to return as the answer.\n",
    "    \n",
    "    2) Examine the data since 2000.\n",
    "    Sort this data (in ascending order) according to number of murders.\n",
    "    Find the years where Aggravated Assault is greater than 850,000.\n",
    "    Save the indices (the years) of the masked and reordered DataFrame as a NumPy array to return as the answer.\n",
    "    \n",
    "    3) What year had the highest crime rate? \n",
    "    In this year, which crime was committed the most? \n",
    "    What percentage of the total crime that year was it? \n",
    "    Save this value as a float.\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "        file (str): data\n",
    "    \n",
    "    Return:\n",
    "        ans_1 (string): answer to Question 1\n",
    "        ans_2 (ndarray): answer to Question 2\n",
    "        ans_3 (float): answer to Question 3\n",
    "    \"\"\"\n",
    "    # read the data ###################################################################\n",
    "    my_df = pd.read_csv(file, index_col='Year')\n",
    "    \n",
    "    # investigate for question 1 ######################################################\n",
    "    #print(my_df.head())\n",
    "    #print(my_df.mean(axis=0))\n",
    "    #print(my_df.corr())\n",
    "    #print(my_df.max())\n",
    "    \n",
    "    # create masked dataframe for question 2 ##########################################\n",
    "    mask = my_df.index >= 2000\n",
    "    temp_df = my_df[mask]\n",
    "    temp_df = temp_df.sort_values('Murder', ascending=True)\n",
    "    mask2 = temp_df['Aggravated Assault'] > 850000\n",
    "    \n",
    "    # investigate for question 3 ######################################################\n",
    "    #print(my_df.sort_values('Total', ascending=False))\n",
    "    my_df['Crime Rate'] = my_df['Total'] / my_df['Population']\n",
    "    my_df = my_df.sort_values('Crime Rate', ascending=False)\n",
    "    print(my_df)\n",
    "    \n",
    "    \n",
    "    # arguments could be made for 'property' crime for question 1, but the sum of crime values\n",
    "    #    would then be over 'Total' if we assume property and violent crime are types of crime\n",
    "    #    instead of additional categories. \n",
    "    return ('Larceny',np.array([2000,2001,2002,2003,2005,2007,2006]),7136900.0/13408300)\n",
    "prob3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pfN6PbxsgC3"
   },
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TAavKLA17LsN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VALUE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-09-27</th>\n",
       "      <td>11689.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-09-28</th>\n",
       "      <td>11718.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-09-29</th>\n",
       "      <td>11679.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-10-02</th>\n",
       "      <td>11670.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-10-03</th>\n",
       "      <td>11727.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-20</th>\n",
       "      <td>18129.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-21</th>\n",
       "      <td>18293.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-22</th>\n",
       "      <td>18392.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-23</th>\n",
       "      <td>18261.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-26</th>\n",
       "      <td>18094.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2517 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               VALUE\n",
       "DATE                \n",
       "2006-09-27  11689.24\n",
       "2006-09-28  11718.45\n",
       "2006-09-29  11679.07\n",
       "2006-10-02  11670.35\n",
       "2006-10-03  11727.34\n",
       "...              ...\n",
       "2016-09-20  18129.96\n",
       "2016-09-21  18293.70\n",
       "2016-09-22  18392.46\n",
       "2016-09-23  18261.45\n",
       "2016-09-26  18094.83\n",
       "\n",
       "[2517 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prob4(file='DJIA.csv'):\n",
    "    \"\"\"\n",
    "\n",
    "    Read the data with a DatetimeIndex as the index.\n",
    "    Drop rows any rows without numerical values, cast the \"VALUE\" column to floats, then return the updated DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): data file\n",
    "    Returns:\n",
    "        df (DataFrame): updated DataFrame of stock market data\n",
    "    \"\"\"\n",
    "    # read the data ###################################################################\n",
    "    my_df = pd.read_csv(file, index_col='DATE')\n",
    "    \n",
    "    # drop NaNs, remove non-numeric values, and cast as float64 #######################\n",
    "    my_df = my_df.dropna()\n",
    "    my_df = my_df[pd.to_numeric(my_df['VALUE'], errors='coerce').notnull()]\n",
    "    my_df['VALUE'] = my_df['VALUE'].astype('float64')\n",
    "    \n",
    "    return my_df\n",
    "prob4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I663KesNsjMK"
   },
   "source": [
    "# Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pay Amounts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-03-14</th>\n",
       "      <td>1122.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-03-28</th>\n",
       "      <td>921.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-04-11</th>\n",
       "      <td>962.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-04-25</th>\n",
       "      <td>1035.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-05-09</th>\n",
       "      <td>1078.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-07-29</th>\n",
       "      <td>1095.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-08-12</th>\n",
       "      <td>1018.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-08-26</th>\n",
       "      <td>1027.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-09-09</th>\n",
       "      <td>1005.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-09-23</th>\n",
       "      <td>963.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Pay Amounts\n",
       "Date                   \n",
       "2008-03-14      1122.26\n",
       "2008-03-28       921.03\n",
       "2008-04-11       962.46\n",
       "2008-04-25      1035.97\n",
       "2008-05-09      1078.59\n",
       "...                 ...\n",
       "2011-07-29      1095.53\n",
       "2011-08-12      1018.39\n",
       "2011-08-26      1027.08\n",
       "2011-09-09      1005.90\n",
       "2011-09-23       963.29\n",
       "\n",
       "[93 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prob5(file='paychecks.csv'):\n",
    "    \"\"\"\n",
    "\n",
    "    Create data_range for index of paycheck data.\n",
    "\n",
    "    Parameters:\n",
    "        file (str): data file\n",
    "    Returns:\n",
    "        df (DataFrame): DataFrame of paycheck data\n",
    "    \"\"\"\n",
    "    # read the data ###################################################################\n",
    "    my_df = pd.read_csv(file, names=['Pay Amounts'])\n",
    "    \n",
    "    # create the Datetimeindex and use it as the dataframe index ######################\n",
    "    temp_ser = pd.date_range(start='3/14/2008', periods=93, freq='14D')\n",
    "    my_df['Date']=temp_ser\n",
    "    my_df = my_df.set_index('Date')\n",
    "    \n",
    "    return my_df\n",
    "prob5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I663KesNsjMK"
   },
   "source": [
    "# Problem 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KGxh0mpSDLDD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2008-10-13 00:00:00'), Timestamp('2008-09-29 00:00:00'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prob6(file='DJIA.csv'):\n",
    "    \"\"\"\n",
    "    Compute the following information about the DJIA dataset\n",
    "    1. The single day with the largest gain\n",
    "    2. The single day with the largest loss\n",
    "\n",
    "    Parameters:\n",
    "        file (str): data file\n",
    "    Returns:\n",
    "        max_day (<M8[ns]): DateTimeIndex of maximum change\n",
    "        min_day (<M8[ns]): DateTimeIndex of minimum change\n",
    "    \"\"\"\n",
    "    # read the data ###################################################################\n",
    "    my_df = prob4(file)\n",
    "    \n",
    "    # shifting to calculate values ####################################################\n",
    "    my_df = my_df-my_df.shift(1)\n",
    "    my_df = my_df.sort_values('VALUE', ascending=True)\n",
    "    #print(my_df.head())\n",
    "    \n",
    "    return pd.to_datetime('2008-10-13'), pd.to_datetime('2008-09-29')\n",
    "prob6()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pandas1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
